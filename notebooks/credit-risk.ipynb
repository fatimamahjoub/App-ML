{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Credit Risk â€” End-to-end Notebook\n",
        "\n",
        "This notebook reproduces the Streamlit app workflow in one place:\n",
        "\n",
        "- Dataset loading (bundled Excel)\n",
        "- Basic analysis / EDA\n",
        "- Cleaning (missing values / duplicates / outliers)\n",
        "- Model training + evaluation\n",
        "- Save / load a trained model artifact\n",
        "- Predict on new inputs\n",
        "\n",
        "> Tip: run this notebook from `streamlit-app-v2/` so paths resolve cleanly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Setup: imports + project path ---\n",
        "from __future__ import annotations\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Make `src/` importable when running from the notebook.\n",
        "APP_DIR = Path.cwd()  # expected: .../streamlit-app-v2\n",
        "if (APP_DIR / \"src\").exists() and str(APP_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(APP_DIR))\n",
        "\n",
        "from src.constants import ARTIFACTS_DIR, DEFAULT_DATASET_PATH, TARGET_COL\n",
        "from src.data_io import read_dataset_from_path\n",
        "from src.cleaning import CleaningConfig, clean_dataframe\n",
        "from src.modeling import MODEL_SPECS, TrainConfig, load_artifact, save_artifact, train_and_evaluate\n",
        "\n",
        "print(\"APP_DIR:\", APP_DIR)\n",
        "print(\"DEFAULT_DATASET_PATH:\", DEFAULT_DATASET_PATH)\n",
        "print(\"TARGET_COL:\", TARGET_COL)\n",
        "print(\"ARTIFACTS_DIR:\", ARTIFACTS_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 1: Load dataset ---\n",
        "\n",
        "df = read_dataset_from_path(DEFAULT_DATASET_PATH)\n",
        "\n",
        "print(\"shape:\", df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 2: Quick analysis / EDA ---\n",
        "\n",
        "# basic info\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\nMissing values (top 20):\")\n",
        "na = df.isna().sum().sort_values(ascending=False)\n",
        "display(na.head(20))\n",
        "\n",
        "print(\"\\nTarget distribution:\")\n",
        "display(df[TARGET_COL].value_counts(dropna=False))\n",
        "\n",
        "# describe numeric\n",
        "num_desc = df.describe(include=[np.number]).T\n",
        "num_desc[\"missing\"] = df.select_dtypes(include=[np.number]).isna().sum().values\n",
        "num_desc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 3: Cleaning ---\n",
        "\n",
        "# This mirrors the Streamlit Cleaning page defaults.\n",
        "clean_cfg = CleaningConfig(\n",
        "    target_col=TARGET_COL,\n",
        "    numeric_missing=\"mean\",\n",
        "    categorical_missing=\"mode\",\n",
        "    drop_duplicates=True,\n",
        "    outlier_method=\"none\",\n",
        "    outlier_cols=[],\n",
        "    zscore_threshold=3.0,\n",
        "    mean_std_k=3.0,\n",
        ")\n",
        "\n",
        "clean_df, clean_report = clean_dataframe(df, clean_cfg)\n",
        "\n",
        "print(\"Rows before:\", df.shape[0])\n",
        "print(\"Rows after: \", clean_df.shape[0])\n",
        "print(\"Nulls before:\", int(df.isna().sum().sum()))\n",
        "print(\"Nulls after: \", int(clean_df.isna().sum().sum()))\n",
        "\n",
        "clean_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 4: Train + evaluate models ---\n",
        "\n",
        "# Identify feature types similarly to the app.\n",
        "feature_cols = [c for c in clean_df.columns if c != TARGET_COL]\n",
        "numeric_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(clean_df[c])]\n",
        "categorical_cols = [c for c in feature_cols if c not in numeric_cols]\n",
        "\n",
        "print(\"#numeric:\", len(numeric_cols), \"#categorical:\", len(categorical_cols))\n",
        "\n",
        "cfg_base = dict(target_col=TARGET_COL, test_size=0.30, random_state=42)\n",
        "\n",
        "runs = {}\n",
        "rows = []\n",
        "for model_id, spec in MODEL_SPECS.items():\n",
        "    pipe, metrics = train_and_evaluate(\n",
        "        clean_df,\n",
        "        numeric_cols=numeric_cols,\n",
        "        categorical_cols=categorical_cols,\n",
        "        cfg=TrainConfig(**cfg_base, model_id=model_id),\n",
        "    )\n",
        "    runs[model_id] = {\"pipeline\": pipe, \"metrics\": metrics}\n",
        "    rows.append(\n",
        "        {\n",
        "            \"model_id\": model_id,\n",
        "            \"model_label\": spec.label,\n",
        "            \"accuracy\": metrics.get(\"accuracy\"),\n",
        "            \"precision\": metrics.get(\"precision\"),\n",
        "            \"recall\": metrics.get(\"recall\"),\n",
        "            \"f1\": metrics.get(\"f1\"),\n",
        "            \"roc_auc\": metrics.get(\"roc_auc\"),\n",
        "        }\n",
        "    )\n",
        "\n",
        "results_df = pd.DataFrame(rows).sort_values(\"f1\", ascending=False)\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick the best model by F1\n",
        "best_id = str(results_df.iloc[0][\"model_id\"])\n",
        "best_label = MODEL_SPECS[best_id].label\n",
        "\n",
        "print(\"Best model:\", best_id, \"(\", best_label, \")\")\n",
        "\n",
        "best = runs[best_id]\n",
        "print(best[\"metrics\"][\"classification_report\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 5: Save + load the trained model artifact ---\n",
        "\n",
        "artifact_path = ARTIFACTS_DIR / \"credit_risk_model.joblib\"\n",
        "\n",
        "metadata = {\n",
        "    \"model_id\": best_id,\n",
        "    \"model_label\": best_label,\n",
        "    \"metrics\": best[\"metrics\"],\n",
        "    \"cleaning_config\": clean_cfg.__dict__,\n",
        "    \"numeric_cols\": numeric_cols,\n",
        "    \"categorical_cols\": categorical_cols,\n",
        "}\n",
        "\n",
        "save_artifact(artifact_path, pipeline=best[\"pipeline\"], metadata=metadata)\n",
        "print(\"Saved:\", artifact_path)\n",
        "\n",
        "loaded = load_artifact(artifact_path)\n",
        "loaded.keys()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Step 6: Predict on a new sample ---\n",
        "\n",
        "pipe = loaded[\"pipeline\"]\n",
        "meta = loaded.get(\"metadata\", {}) or {}\n",
        "\n",
        "# Build a single-row payload using medians/modes from the cleaned dataset\n",
        "payload = {}\n",
        "for c in meta.get(\"numeric_cols\", numeric_cols):\n",
        "    payload[c] = float(pd.to_numeric(clean_df[c], errors=\"coerce\").median())\n",
        "for c in meta.get(\"categorical_cols\", categorical_cols):\n",
        "    # pick the most frequent category (mode)\n",
        "    mode = clean_df[c].dropna().astype(str).mode()\n",
        "    payload[c] = str(mode.iloc[0]) if len(mode) else \"UNKNOWN\"\n",
        "\n",
        "x = pd.DataFrame([payload])\n",
        "pred = pipe.predict(x)[0]\n",
        "\n",
        "# Decode prediction using target_names when available\n",
        "names = (meta.get(\"metrics\") or {}).get(\"target_names\", [])\n",
        "label = names[int(pred)] if isinstance(pred, (int, float)) and int(pred) < len(names) else str(pred)\n",
        "\n",
        "print(\"prediction:\", label)\n",
        "\n",
        "if hasattr(pipe, \"predict_proba\"):\n",
        "    proba = pipe.predict_proba(x)[0]\n",
        "    # show classes + probabilities\n",
        "    classes = []\n",
        "    if hasattr(pipe, \"named_steps\") and \"clf\" in pipe.named_steps and hasattr(pipe.named_steps[\"clf\"], \"classes_\"):\n",
        "        classes = list(pipe.named_steps[\"clf\"].classes_)\n",
        "    elif hasattr(pipe, \"classes_\"):\n",
        "        classes = list(pipe.classes_)\n",
        "\n",
        "    if classes and len(classes) == len(proba):\n",
        "        def _lbl(c):\n",
        "            if isinstance(c, (int, float)) and int(c) < len(names):\n",
        "                return str(names[int(c)])\n",
        "            return str(c)\n",
        "\n",
        "        display(\n",
        "            pd.DataFrame({\"class\": [_lbl(c) for c in classes], \"probability\": [float(p) for p in proba]})\n",
        "            .sort_values(\"probability\", ascending=False)\n",
        "            .reset_index(drop=True)\n",
        "        )\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
